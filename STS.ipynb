{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Semantic Textual Similarity for Korean\n"
      ],
      "metadata": {
        "id": "8hCnA1B7QRwo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import data"
      ],
      "metadata": {
        "id": "z-Fu_ivvQRzD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "esiGYeXTIVe5",
        "outputId": "95d56027-5ed2-4f39-b41b-146f9fc7f674"
      },
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.19.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.5.18.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "import os"
      ],
      "metadata": {
        "id": "X17u4TxWQgZB"
      },
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# seed\n",
        "seed = 7777\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)"
      ],
      "metadata": {
        "id": "DUUwmST90CEW"
      },
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# device type\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f\"# available GPUs : {torch.cuda.device_count()}\")\n",
        "    print(f\"GPU name : {torch.cuda.get_device_name()}\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJSrVKyw0EOk",
        "outputId": "9fdcaee9-fe92-4714-d438-15db980569cd"
      },
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# available GPUs : 1\n",
            "GPU name : Tesla T4\n",
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Given data"
      ],
      "metadata": {
        "id": "6F60gZSXMnA8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dxd6gSy8v1F0",
        "outputId": "a4d2df2b-7ab1-4ddb-a3be-21fdcc9d33ea"
      },
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd \"/content/drive/MyDrive/NLP_project\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RzLQjQ_ywFk-",
        "outputId": "4ecc9058-d772-4f13-f40b-79f6311da7f6"
      },
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/NLP_project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_CUR_DIR = os.path.abspath(os.curdir)\n",
        "print(f\"My current directory : {_CUR_DIR}\")\n",
        "_DATA_DIR = os.path.join(_CUR_DIR, \"klue-sts-v1.1\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wik2PXw-wNN5",
        "outputId": "730881d2-bd9d-4f3e-ac3c-f7ba30e0930f"
      },
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My current directory : /content/drive/MyDrive/NLP_project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train0 = pd.read_json('./klue-sts-v1.1/klue-sts-v1.1_train.json')\n",
        "df_test0 = pd.read_json('./klue-sts-v1.1/klue-sts-v1.1_dev.json')"
      ],
      "metadata": {
        "id": "_X-5km4NwTQJ"
      },
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_train0.shape, df_test.shape)\n",
        "print(df_train0.columns)\n",
        "print(df_test.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hq9Fsmcz0VC0",
        "outputId": "550b4f87-0561-4ccd-b152-1a6b83fab90d"
      },
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(11668, 6) (519, 2)\n",
            "Index(['guid', 'source', 'sentence1', 'sentence2', 'labels', 'annotations'], dtype='object')\n",
            "Index(['sentence', 'label'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Collected data"
      ],
      "metadata": {
        "id": "cNZVS1adMeFe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "bhuXo-IiMf1E"
      },
      "execution_count": 187,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualization"
      ],
      "metadata": {
        "id": "P1I9uYIMM3Kf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train0['labels'].map(lambda x: x['real-label']).hist(bins=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "id": "_oZuc4_sDF5B",
        "outputId": "a1d55367-ded2-43a2-87ce-f58e732cab07"
      },
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f2f8e511d50>"
            ]
          },
          "metadata": {},
          "execution_count": 188
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train0['sentence1'].map(lambda x: len(x)).hist()"
      ],
      "metadata": {
        "id": "mZHw4B4Ls3et",
        "outputId": "4a386f14-11d8-4dcc-90ed-8b47d3491034",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        }
      },
      "execution_count": 189,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f2f8e77e210>"
            ]
          },
          "metadata": {},
          "execution_count": 189
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAD4CAYAAAAO9oqkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAU/UlEQVR4nO3df4xd5X3n8fenOAHCdLEJdJa1rYXdWIlIvCEwAqJU1Ri2YCCK+SONqFAxWa/8D+3SlaXGNMqiJkQi2lAapIZdy7gxUTaEpcliQTbUdRhF+QNCnB+YH2E9ASfYIriNjVsHmnS63/3jPuNejIf5PXcu+35Jo3vOc5577vc8npmPz3POvZOqQpL0/7df63UBkqTeMwwkSYaBJMkwkCRhGEiSgCW9LuCNnHnmmXXWWWdx2mmn9bqUGfvFL37Rt/Vbe29Ye+/0c/3dte/evftvq+qsae2gqhbt14UXXliPPPJI9bN+rt/ae8Pae6ef6++uHfhuTfP3rdNEkiTDQJJkGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIElikX8cRb86Z/NDx5Y3rR7jhq71+bTvtqsX5HUkvfl4ZiBJMgwkSYaBJAnDQJKEYSBJYophkGRpkvuT/CjJM0nen+SMJDuT7G2Py1rfJLkzyWiSJ5Jc0LWf9a3/3iTr5+ugJEnTM9Uzg88B36iqdwHvBZ4BNgO7qmoVsKutA1wJrGpfG4G7AJKcAdwCXAxcBNwyHiCSpN6aNAySnA78FnA3QFX9qqpeBtYB21u37cA1bXkdcE/7gzuPAkuTnA1cAeysqkNVdRjYCayd06ORJM1IOn8h7Q06JOcDW4Cn6ZwV7AZuAg5U1dLWJ8Dhqlqa5EHgtqr6dtu2C/gYMAycUlW3tvZPAK9W1WePe72NdM4oGBwcvHDr1q0MDAzM0eEujD0HjhxbHjwVXnp1YV539fLT53R/R48e7buxH2ftvdHPtUN/199d+5o1a3ZX1dB0nj+VdyAvAS4A/qCqHkvyOf55SgiAqqokb5wqU1RVW+iED0NDQzUwMMDw8PBc7HrB3HDcO5Bv37Mwb/Ted93wnO5vZGSk78Z+nLX3Rj/XDv1d/2xrn8o1g/3A/qp6rK3fTyccXmrTP7THg237AWBl1/NXtLaJ2iVJPTZpGFTVz4AXkryzNV1GZ8poBzB+R9B64IG2vAO4vt1VdAlwpKpeBB4GLk+yrF04vry1SZJ6bKrzF38AfCnJW4HngI/SCZL7kmwAfgJ8pPX9OnAVMAq80vpSVYeSfAp4vPX7ZFUdmpOjkCTNypTCoKp+AJzoYsRlJ+hbwI0T7GcbsG06BUqS5p/vQJYkGQaSJMNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiSmGAZJ9iXZk+QHSb7b2s5IsjPJ3va4rLUnyZ1JRpM8keSCrv2sb/33Jlk/P4ckSZqu6ZwZrKmq86tqqK1vBnZV1SpgV1sHuBJY1b42AndBJzyAW4CLgYuAW8YDRJLUW7OZJloHbG/L24FrutrvqY5HgaVJzgauAHZW1aGqOgzsBNbO4vUlSXMkVTV5p+R54DBQwH+vqi1JXq6qpW17gMNVtTTJg8BtVfXttm0X8DFgGDilqm5t7Z8AXq2qzx73WhvpnFEwODh44datWxkYGJibo10gew4cObY8eCq89OrCvO7q5afP6f6OHj3ad2M/ztp7o59rh/6uv7v2NWvW7O6axZmSJVPs95tVdSDJbwA7k/yoe2NVVZLJU2UKqmoLsAVgaGioBgYGGB4enotdL5gbNj90bHnT6jFu3zPVYZ6dfdcNz+n+RkZG+m7sx1l7b/Rz7dDf9c+29ilNE1XVgfZ4EPganTn/l9r0D+3xYOt+AFjZ9fQVrW2idklSj00aBklOS/Lr48vA5cCTwA5g/I6g9cADbXkHcH27q+gS4EhVvQg8DFyeZFm7cHx5a5Mk9dhU5i8Gga91LguwBPgfVfWNJI8D9yXZAPwE+Ejr/3XgKmAUeAX4KEBVHUryKeDx1u+TVXVozo5EkjRjk4ZBVT0HvPcE7T8HLjtBewE3TrCvbcC26ZcpSZpPvgNZkmQYSJIMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSmEYYJDkpyfeTPNjWz03yWJLRJF9J8tbWfnJbH23bz+nax82t/dkkV8z1wUiSZmY6ZwY3Ac90rX8GuKOq3gEcBja09g3A4dZ+R+tHkvOAa4F3A2uBzyc5aXblS5LmwpTCIMkK4Gpga1sPcClwf+uyHbimLa9r67Ttl7X+64B7q+qXVfU8MApcNBcHIUmanSVT7PdnwB8Bv97W3w68XFVjbX0/sLwtLwdeAKiqsSRHWv/lwKNd++x+zjFJNgIbAQYHBzl69CgjIyNTPZ5FYdPqsWPLg6e+dn0+zfU49ePYj7P23ujn2qG/659t7ZOGQZIPAgeraneS4Rm/0hRV1RZgC8DQ0FANDAwwPDzvLzunbtj80LHlTavHuH3PVDN3dvZdNzyn+xsZGem7sR9n7b3Rz7VDf9c/29qn8lvqA8CHklwFnAL8C+BzwNIkS9rZwQrgQOt/AFgJ7E+yBDgd+HlX+7ju50iSemjSawZVdXNVraiqc+hcAP5mVV0HPAJ8uHVbDzzQlne0ddr2b1ZVtfZr291G5wKrgO/M2ZFIkmZsNvMXHwPuTXIr8H3g7tZ+N/DFJKPAIToBQlU9leQ+4GlgDLixqv5pFq8vSZoj0wqDqhoBRtryc5zgbqCq+gfgdyZ4/qeBT0+3SEnS/PIdyJIkw0CSZBhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAksQUwiDJKUm+k+SHSZ5K8iet/dwkjyUZTfKVJG9t7Se39dG2/Zyufd3c2p9NcsV8HZQkaXqmcmbwS+DSqnovcD6wNsklwGeAO6rqHcBhYEPrvwE43NrvaP1Ich5wLfBuYC3w+SQnzeXBSJJmZtIwqI6jbfUt7auAS4H7W/t24Jq2vK6t07ZfliSt/d6q+mVVPQ+MAhfNyVFIkmZlyVQ6tf/B7wbeAfw58GPg5aoaa132A8vb8nLgBYCqGktyBHh7a3+0a7fdz+l+rY3ARoDBwUGOHj3KyMjI9I6qxzatHju2PHjqa9fn01yPUz+O/Thr741+rh36u/7Z1j6lMKiqfwLOT7IU+Brwrhm/4uSvtQXYAjA0NFQDAwMMDw/P18vNixs2P3RsedPqMW7fM6VhnrV91w3P6f5GRkb6buzHWXtv9HPt0N/1z7b2ad1NVFUvA48A7weWJhn/LbcCONCWDwArAdr204Gfd7ef4DmSpB6ayt1EZ7UzApKcCvw28AydUPhw67YeeKAt72jrtO3frKpq7de2u43OBVYB35mrA5EkzdxU5i/OBra36wa/BtxXVQ8meRq4N8mtwPeBu1v/u4EvJhkFDtG5g4iqeirJfcDTwBhwY5t+kiT12KRhUFVPAO87QftznOBuoKr6B+B3JtjXp4FPT79MSdJ88h3IkiTDQJJkGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEliip9aqv5wTtenpc6FTavHXvMJrG9k321Xz+lrS1pYnhlIkgwDSZJhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJTCEMkqxM8kiSp5M8leSm1n5Gkp1J9rbHZa09Se5MMprkiSQXdO1rfeu/N8n6+TssSdJ0TOXMYAzYVFXnAZcANyY5D9gM7KqqVcCutg5wJbCqfW0E7oJOeAC3ABcDFwG3jAeIJKm3Jg2Dqnqxqr7Xlv8eeAZYDqwDtrdu24Fr2vI64J7qeBRYmuRs4ApgZ1UdqqrDwE5g7ZwejSRpRlJVU++cnAN8C3gP8NOqWtraAxyuqqVJHgRuq6pvt227gI8Bw8ApVXVra/8E8GpVffa419hI54yCwcHBC7du3crAwMBsjnHB7Tlw5Njy4Knw0qs9LGYWplP76uWnz28x03T06NG++74ZZ+2908/1d9e+Zs2a3VU1NJ3nT/nPXiYZAP4S+MOq+rvO7/+OqqokU0+VN1BVW4AtAENDQzUwMMDw8PBc7HrBdP+pyE2rx7h9T3/+ddHp1L7vuuH5LWaaRkZG+u77Zpy1904/1z/b2qd0N1GSt9AJgi9V1Vdb80tt+of2eLC1HwBWdj19RWubqF2S1GNTuZsowN3AM1X1p12bdgDjdwStBx7oar++3VV0CXCkql4EHgYuT7KsXTi+vLVJknpsKnMAHwB+D9iT5Aet7Y+B24D7kmwAfgJ8pG37OnAVMAq8AnwUoKoOJfkU8Hjr98mqOjQnRyFJmpVJw6BdCM4Emy87Qf8CbpxgX9uAbdMpUJI0/3wHsiTJMJAkGQaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJDGFMEiyLcnBJE92tZ2RZGeSve1xWWtPkjuTjCZ5IskFXc9Z3/rvTbJ+fg5HkjQTUzkz+AKw9ri2zcCuqloF7GrrAFcCq9rXRuAu6IQHcAtwMXARcMt4gEiSem/SMKiqbwGHjmteB2xvy9uBa7ra76mOR4GlSc4GrgB2VtWhqjoM7OT1ASNJ6pElM3zeYFW92JZ/Bgy25eXAC1399re2idpfJ8lGOmcVDA4OcvToUUZGRmZYZm9sWj12bHnw1Neu95Pp1L7Y/o368ftmnLX3Tj/XP9vaZxoGx1RVJanZ7qdrf1uALQBDQ0M1MDDA8PDwXO1+Qdyw+aFjy5tWj3H7nlkPc09Mp/Z91w3PbzHTNDIy0nffN+OsvXf6uf7Z1j7Tu4leatM/tMeDrf0AsLKr34rWNlG7JGkRmGkY7ADG7whaDzzQ1X59u6voEuBIm056GLg8ybJ24fjy1iZJWgQmnQNI8mVgGDgzyX46dwXdBtyXZAPwE+AjrfvXgauAUeAV4KMAVXUoyaeAx1u/T1bV8Rel1cfO6ZoaW0j7bru6J68rvdlMGgZV9bsTbLrsBH0LuHGC/WwDtk2rOknSgvAdyJIkw0CSZBhIkjAMJEkYBpIk5uAdyItZr253lKR+45mBJMkwkCS9yaeJ9OY30VTgptVjr/nAwPngu5/1ZuKZgSTJMJAkGQaSJAwDSRKGgSQJ7yaSZmy+3tQ42Z1Q3sWk+eCZgSTJMJAkGQaSJLxmIPWdXn4Ao9cr3rw8M5AkGQaSJMNAkoRhIEnCMJAk0YMwSLI2ybNJRpNsXujXlyS93oLeWprkJODPgd8G9gOPJ9lRVU8vZB2SZmay21rn648KeUvr/FvoM4OLgNGqeq6qfgXcC6xb4BokScdJVS3ciyUfBtZW1X9s678HXFxVv9/VZyOwsa2+E/g58LcLVuTcO5P+rd/ae8Pae6ef6++u/V9X1VnTefKiewdyVW0BtoyvJ/luVQ31sKRZ6ef6rb03rL13+rn+2da+0NNEB4CVXesrWpskqYcWOgweB1YlOTfJW4FrgR0LXIMk6TgLOk1UVWNJfh94GDgJ2FZVT03ytC2TbF/s+rl+a+8Na++dfq5/VrUv6AVkSdLi5DuQJUmGgSRpkYdBP310RZKVSR5J8nSSp5Lc1NrPSLIzyd72uKzXtU4kyUlJvp/kwbZ+bpLH2vh/pV30X3SSLE1yf5IfJXkmyfv7bNz/c/ueeTLJl5OcsljHPsm2JAeTPNnVdsKxTsed7RieSHJB7yqfsPb/2r5vnkjytSRLu7bd3Gp/NskVvan6WC2vq71r26YkleTMtj6jcV+0YdD10RVXAucBv5vkvN5W9YbGgE1VdR5wCXBjq3czsKuqVgG72vpidRPwTNf6Z4A7quodwGFgQ0+qmtzngG9U1buA99I5hr4Y9yTLgf8EDFXVe+jcWHEti3fsvwCsPa5torG+EljVvjYCdy1QjRP5Aq+vfSfwnqr6d8D/AW4GaD+71wLvbs/5fPud1Ctf4PW1k2QlcDnw067mGY37og0D+uyjK6rqxar6Xlv+ezq/kJbTqXl767YduKY3Fb6xJCuAq4GtbT3ApcD9rcuirD3J6cBvAXcDVNWvqupl+mTcmyXAqUmWAG8DXmSRjn1VfQs4dFzzRGO9DrinOh4FliY5e2Eqfb0T1V5Vf1VVY231UTrvfYJO7fdW1S+r6nlglM7vpJ6YYNwB7gD+COi+E2hG476Yw2A58ELX+v7WtuglOQd4H/AYMFhVL7ZNPwMGe1TWZP6MzjfV/23rbwde7vpBWazjfy7wN8BftCmurUlOo0/GvaoOAJ+l8z+7F4EjwG76Y+zHTTTW/fYz/B+A/92WF33tSdYBB6rqh8dtmlHtizkM+lKSAeAvgT+sqr/r3lad+3gX3b28ST4IHKyq3b2uZQaWABcAd1XV+4BfcNyU0GIdd4A2v76OTqj9K+A0TjAd0C8W81i/kSQfpzPV+6Ve1zIVSd4G/DHwX+Zqn4s5DPruoyuSvIVOEHypqr7aml8aP0Vrjwd7Vd8b+ADwoST76EzHXUpnHn5pm7qAxTv++4H9VfVYW7+fTjj0w7gD/Hvg+ar6m6r6R+CrdP49+mHsx0001n3xM5zkBuCDwHX1z2+8Wuy1/1s6/4H4Yfu5XQF8L8m/ZIa1L+Yw6KuPrmhz7HcDz1TVn3Zt2gGsb8vrgQcWurbJVNXNVbWiqs6hM87frKrrgEeAD7dui7X2nwEvJHlna7oMeJo+GPfmp8AlSd7WvofG61/0Y99lorHeAVzf7m65BDjSNZ20KCRZS2d69ENV9UrXph3AtUlOTnIunYux3+lFjSdSVXuq6jeq6pz2c7sfuKD9PMxs3Ktq0X4BV9G5wv9j4OO9rmeSWn+TzunxE8AP2tdVdObedwF7gb8Gzuh1rZMcxzDwYFv+N3R+AEaB/wmc3Ov6Jqj5fOC7bez/F7Csn8Yd+BPgR8CTwBeBkxfr2ANfpnNt4x/bL6ANE401EDp3BP4Y2EPnjqnFVvsonfn18Z/Z/9bV/+Ot9meBKxdb7cdt3wecOZtx9+MoJEmLeppIkrRADANJkmEgSTIMJEkYBpIkDANJEoaBJAn4f6K0oCSfzbLRAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train0['sentence2'].map(lambda x: len(x)).hist()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "id": "qVQ_9azlPoBy",
        "outputId": "8204372c-9504-47fb-e4fc-d7a0bcca7cbb"
      },
      "execution_count": 190,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f2f8ea5c590>"
            ]
          },
          "metadata": {},
          "execution_count": 190
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data preprocess"
      ],
      "metadata": {
        "id": "ySxzpHXJQR1s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler, random_split\n",
        "from transformers import BertTokenizer"
      ],
      "metadata": {
        "id": "oTmY-YL5Lo_1"
      },
      "execution_count": 191,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocess"
      ],
      "metadata": {
        "id": "2L3Qg_QqFKpS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Preprocess:\n",
        "    def __init__(self,train,test):\n",
        "        self.train=train\n",
        "        self.test=test\n",
        "\n",
        "    def BERT_baseline(self):\n",
        "        sentence = self.train.apply(lambda x: x['sentence1']+' [SEP] '+x['sentence2'],axis=1)\n",
        "        label = self.train['labels'].map(lambda x: round(x['real-label']))\n",
        "        train = pd.concat([sentence,label],axis=1)\n",
        "        train.columns = ['sentence','label']\n",
        "        sentence = self.test.apply(lambda x: x['sentence1']+' [SEP] '+x['sentence2'],axis=1)\n",
        "        label = self.test['labels'].map(lambda x: round(x['real-label']))\n",
        "        test = pd.concat([sentence,label],axis=1)\n",
        "        test.columns = ['sentence','label']\n",
        "        return train, test\n",
        "\n",
        "    def BERT_floor(self):\n",
        "        sentence = self.train.apply(lambda x: x['sentence1']+' [SEP] '+x['sentence2'],axis=1)\n",
        "        label = self.train['labels'].map(lambda x: int(x['real-label']))\n",
        "        train = pd.concat([sentence,label],axis=1)\n",
        "        train.columns = ['sentence','label']\n",
        "        sentence = self.test.apply(lambda x: x['sentence1']+' [SEP] '+x['sentence2'],axis=1)\n",
        "        label = self.test['labels'].map(lambda x: int(x['real-label']))\n",
        "        test = pd.concat([sentence,label],axis=1)\n",
        "        test.columns = ['sentence','label']\n",
        "        return train, test"
      ],
      "metadata": {
        "id": "jVwblZvZQXGI"
      },
      "execution_count": 217,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pp=Preprocess(df_train0,df_test0)"
      ],
      "metadata": {
        "id": "cKSuoWq1e1Tt"
      },
      "execution_count": 218,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train, df_test = pp.BERT_baseline()"
      ],
      "metadata": {
        "id": "u7hYDHWGCm0C"
      },
      "execution_count": 194,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train, df_test = pp.BERT_floor()"
      ],
      "metadata": {
        "id": "9va5TBFqyc9z"
      },
      "execution_count": 219,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "fu8-BMqCC93p",
        "outputId": "d4d3766d-815b-4721-969d-8de33932fb5b"
      },
      "execution_count": 220,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                            sentence  label\n",
              "0  숙소 위치는 찾기 쉽고 일반적인 한국의 반지하 숙소입니다. [SEP] 숙박시설의 위...      3\n",
              "1  위반행위 조사 등을 거부·방해·기피한 자는 500만원 이하 과태료 부과 대상이다. ...      0\n",
              "2  회사가 보낸 메일은 이 지메일이 아니라 다른 지메일 계정으로 전달해줘. [SEP] ...      0\n",
              "3  긴급 고용안정지원금은 지역고용대응 등 특별지원금, 지자체별 소상공인 지원사업, 취업...      0\n",
              "4  호스트의 답장이 늦으나, 개선될 것으로 보입니다. [SEP] 호스트 응답이 늦었지만...      4"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ae9d9db2-4416-4eb8-bcbb-aba651d0a239\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>숙소 위치는 찾기 쉽고 일반적인 한국의 반지하 숙소입니다. [SEP] 숙박시설의 위...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>위반행위 조사 등을 거부·방해·기피한 자는 500만원 이하 과태료 부과 대상이다. ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>회사가 보낸 메일은 이 지메일이 아니라 다른 지메일 계정으로 전달해줘. [SEP] ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>긴급 고용안정지원금은 지역고용대응 등 특별지원금, 지자체별 소상공인 지원사업, 취업...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>호스트의 답장이 늦으나, 개선될 것으로 보입니다. [SEP] 호스트 응답이 늦었지만...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ae9d9db2-4416-4eb8-bcbb-aba651d0a239')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ae9d9db2-4416-4eb8-bcbb-aba651d0a239 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ae9d9db2-4416-4eb8-bcbb-aba651d0a239');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 220
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train['label'].value_counts()"
      ],
      "metadata": {
        "id": "L0TceNfiy4ba",
        "outputId": "69e44b08-a776-4e0d-c709-2a05e60a54f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 229,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    4350\n",
              "3    2852\n",
              "4    2705\n",
              "1     906\n",
              "2     810\n",
              "5      45\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 229
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Augmentation"
      ],
      "metadata": {
        "id": "IAIZB6y0kIyH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "nMHTC1mtkLKp"
      },
      "execution_count": 229,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset"
      ],
      "metadata": {
        "id": "MmsEdtqtFHCu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Dataset\n",
        "class CustomDataset(Dataset):\n",
        "    \"\"\"\n",
        "    - input_data: list of string\n",
        "    - target_data: list of int\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, input_data:list, target_data:list) -> None:\n",
        "        self.X = input_data\n",
        "        self.Y = target_data\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        return self.X[index], self.Y[index]"
      ],
      "metadata": {
        "id": "75QHeIiL-_73"
      },
      "execution_count": 230,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = CustomDataset(df_train.sentence.to_list(), df_train.label.to_list())\n",
        "test_dataset = CustomDataset(df_test.sentence.to_list(), df_test.label.to_list())\n",
        "print(f\"Train Dataset len: {len(train_dataset)}\")\n",
        "print(f\"Train Dataset 1st element: {train_dataset[0]}\")\n",
        "\n",
        "print(f\"Test Dataset len: {len(test_dataset)}\")\n",
        "print(f\"Test Dataset 1st element: {test_dataset[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHRSCsPX_XiU",
        "outputId": "ab01a0dd-3677-4688-94dd-fee1c871adb8"
      },
      "execution_count": 231,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Dataset len: 11668\n",
            "Train Dataset 1st element: ('숙소 위치는 찾기 쉽고 일반적인 한국의 반지하 숙소입니다. [SEP] 숙박시설의 위치는 쉽게 찾을 수 있고 한국의 대표적인 반지하 숙박시설입니다.', 3)\n",
            "Test Dataset len: 519\n",
            "Test Dataset 1st element: ('무엇보다도 호스트분들이 너무 친절하셨습니다. [SEP] 무엇보다도, 호스트들은 매우 친절했습니다.', 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DataLoader"
      ],
      "metadata": {
        "id": "bXlXNh47E7_a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_sample = df_train.shape[0]\n",
        "n_train = int(n_sample*0.9)\n",
        "n_valid = n_sample-n_train\n",
        "train_dataset, valid_dataset = random_split(train_dataset, [n_train, n_valid])\n",
        "\n",
        "print(f\"Train dataset len: {len(train_dataset)}\")\n",
        "print(f\"Valid dataset len: {len(valid_dataset)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N3VF0hHy_qbg",
        "outputId": "920d4edd-5ede-4279-c4f0-ef0b6619322c"
      },
      "execution_count": 232,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dataset len: 10501\n",
            "Valid dataset len: 1167\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom collate_fn \n",
        "def custom_collate_fn(batch):\n",
        "    \"\"\"\n",
        "    한 배치 내 문장들을 tokenizing 한 후 텐서로 변환함. \n",
        "    이때, dynamic padding (즉, 같은 배치 내 토큰의 개수가 동일할 수 있도록, 부족한 문장에 [PAD] 토큰을 추가하는 작업)을 적용\n",
        "    \n",
        "    한 배치 내 레이블(target)은 텐서화 함.\n",
        "    \n",
        "    - batch: list of tuples (input_data(string), target_data(int))\n",
        "    \"\"\"\n",
        "    input_list, target_list = [], []\n",
        "\n",
        "    tokenizer_bert = BertTokenizer.from_pretrained(\"klue/bert-base\")\n",
        "    \n",
        "    for _input, _target in batch:\n",
        "        input_list.append(_input)\n",
        "        target_list.append(_target)\n",
        "    \n",
        "    tensorized_input = tokenizer_bert(\n",
        "        input_list,\n",
        "        add_special_tokens=True,\n",
        "        padding=\"longest\", # 배치내 가장 긴 문장을 기준으로 부족한 문장은 [PAD] 토큰을 추가\n",
        "        truncation=True, # max_length를 넘는 문장은 이 후 토큰을 제거함\n",
        "        max_length=512,\n",
        "        return_tensors='pt' # 토크나이즈된 결과 값을 텐서 형태로 반환\n",
        "    )\n",
        "    \n",
        "    tensorized_label = torch.tensor(target_list)\n",
        "    \n",
        "    return tensorized_input, tensorized_label"
      ],
      "metadata": {
        "id": "tOWkWV9L_M-z"
      },
      "execution_count": 233,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DataLoader 구현\n",
        "train_batch_size = 32\n",
        "valid_batch_size = 64\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size = train_batch_size,\n",
        "    sampler = RandomSampler(train_dataset),\n",
        "    collate_fn = custom_collate_fn\n",
        ")\n",
        "\n",
        "valid_dataloader = DataLoader(\n",
        "    valid_dataset,\n",
        "    batch_size = valid_batch_size,\n",
        "    sampler = SequentialSampler(valid_dataset),\n",
        "    collate_fn = custom_collate_fn\n",
        ")\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size = valid_batch_size,\n",
        "    sampler = SequentialSampler(test_dataset),\n",
        "    collate_fn = custom_collate_fn\n",
        ")\n",
        "print(f\"Train dataloader # steps: {len(train_dataloader)}\")\n",
        "print(f\"Valid dataloader # steps: {len(valid_dataloader)}\")\n",
        "print(f\"Test dataloader # steps: {len(test_dataloader)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afNNT7crFH28",
        "outputId": "41458dba-43ff-4bd3-b86b-3d4709fee6da"
      },
      "execution_count": 234,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dataloader # steps: 329\n",
            "Valid dataloader # steps: 19\n",
            "Test dataloader # steps: 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "5CZkR0r1QSBI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertModel, BertForSequenceClassification\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.optim import AdamW\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "import torch.nn as nn\n",
        "from transformers import get_linear_schedule_with_warmup, get_constant_schedule"
      ],
      "metadata": {
        "id": "gmpdF8q-IEUg"
      },
      "execution_count": 235,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initializer"
      ],
      "metadata": {
        "id": "fb4K-Oo8Fc7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def initializer(train_dataloader, epochs=2):\n",
        "    \"\"\"\n",
        "    모델, 옵티마이저, 스케쥴러 초기화\n",
        "    \"\"\"\n",
        "    \n",
        "    model = CustomClassifier(hidden_size=768, n_label=6)\n",
        "\n",
        "    optimizer = AdamW(\n",
        "        model.parameters(), # update 대상 파라미터를 입력\n",
        "        lr=2e-5,\n",
        "        eps=1e-8\n",
        "    )\n",
        "    \n",
        "    total_steps = len(train_dataloader) * epochs\n",
        "    print(f\"Total train steps with {epochs} epochs: {total_steps}\")\n",
        "\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer, \n",
        "        num_warmup_steps = 0, # 여기서는 warmup을 사용하지 않는다.\n",
        "        num_training_steps = total_steps\n",
        "    )\n",
        "\n",
        "    return model, optimizer, scheduler"
      ],
      "metadata": {
        "id": "i4omKhdSQfpJ"
      },
      "execution_count": 236,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### save_checkpoint"
      ],
      "metadata": {
        "id": "NwgtRhD4FhSr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_checkpoint(path, model, optimizer, scheduler, epoch, loss):\n",
        "    file_name = f'{path}/model.ckpt.{epoch}'\n",
        "    \n",
        "    torch.save(\n",
        "        {\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'scheduler_state_dict': scheduler.state_dict(),\n",
        "            'loss' : loss\n",
        "        }, \n",
        "        file_name\n",
        "    )\n",
        "    \n",
        "    print(f\"Saving epoch {epoch} checkpoint at {file_name}\")"
      ],
      "metadata": {
        "id": "xjTgaMgaJLIo"
      },
      "execution_count": 237,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### validate"
      ],
      "metadata": {
        "id": "fBx_gbpYFmSU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(model, valid_dataloader):\n",
        "   \n",
        "    # 모델을 evaluate 모드로 설정 & device 할당\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "    \n",
        "    total_loss, total_acc= 0,0\n",
        "        \n",
        "    for step, batch in enumerate(valid_dataloader):\n",
        "        \n",
        "        # tensor 연산 전, 각 tensor에 device 할당\n",
        "        batch = tuple(item.to(device) for item in batch)\n",
        "            \n",
        "        batch_input, batch_label = batch\n",
        "            \n",
        "        # gradient 계산하지 않음\n",
        "        with torch.no_grad():\n",
        "            logits = model(**batch_input)\n",
        "            \n",
        "        # loss\n",
        "        loss = loss_fct(logits, batch_label)\n",
        "        total_loss += loss.item()\n",
        "        \n",
        "        # accuracy\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        preds = torch.argmax(probs, dim=1).flatten()\n",
        "        acc = (preds == batch_label).cpu().numpy().mean()\n",
        "        total_acc+=acc\n",
        "    \n",
        "    total_loss = total_loss/(step+1)\n",
        "    total_acc = total_acc/(step+1)*100\n",
        "\n",
        "    return total_loss, total_acc"
      ],
      "metadata": {
        "id": "C21fot6EJNSL"
      },
      "execution_count": 238,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Custom Models"
      ],
      "metadata": {
        "id": "eX4B11HRFonl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### BERT/baseline"
      ],
      "metadata": {
        "id": "1C0ChIz5wOix"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Classifer\n",
        "class CustomClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden_size: int, n_label: int, freeze_base: bool = False):\n",
        "        super(CustomClassifier, self).__init__()\n",
        "\n",
        "        self.bert = BertModel.from_pretrained(\"klue/bert-base\")\n",
        "\n",
        "        if freeze_base:\n",
        "            for param in self.bert.parameters():\n",
        "                param.requires_grad=False\n",
        "\n",
        "        dropout_rate = 0.1\n",
        "        linear_layer_hidden_size = 6\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "        nn.Linear(hidden_size, linear_layer_hidden_size),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(dropout_rate),\n",
        "        nn.Linear(linear_layer_hidden_size, n_label)\n",
        "        )\n",
        "\n",
        "    \n",
        "\n",
        "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None):\n",
        "\n",
        "        outputs = self.bert(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "        )\n",
        "\n",
        "        last_hidden_states = outputs[0] # last hidden states (batch_size, sequence_len, hidden_size)\n",
        "        cls_token_last_hidden_states = last_hidden_states[:,0,:] # (batch_size, first_token, hidden_size)\n",
        "\n",
        "        logits = self.classifier(cls_token_last_hidden_states)\n",
        "\n",
        "        return logits"
      ],
      "metadata": {
        "id": "7Y47FP6B_QmC"
      },
      "execution_count": 239,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### BERT/BertForSequenceClassification"
      ],
      "metadata": {
        "id": "Agz2GrpAwRcy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Classifer\n",
        "class CustomClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden_size: int, n_label: int, freeze_base: bool = False):\n",
        "        super(CustomClassifier, self).__init__()\n",
        "\n",
        "        self.bert = BertForSequenceClassification.from_pretrained(\"klue/bert-base\")\n",
        "\n",
        "        if freeze_base:\n",
        "            for param in self.bert.parameters():\n",
        "                param.requires_grad=False\n",
        "\n",
        "        dropout_rate = 0.1\n",
        "        linear_layer_hidden_size = 6\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "        nn.Linear(hidden_size, linear_layer_hidden_size),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(dropout_rate),\n",
        "        nn.Linear(linear_layer_hidden_size, n_label)\n",
        "        )\n",
        "\n",
        "    \n",
        "\n",
        "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None):\n",
        "\n",
        "        outputs = self.bert(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "        )\n",
        "\n",
        "        last_hidden_states = outputs[0] # last hidden states (batch_size, sequence_len, hidden_size)\n",
        "        cls_token_last_hidden_states = last_hidden_states[:,0,:] # (batch_size, first_token, hidden_size)\n",
        "\n",
        "        logits = self.classifier(cls_token_last_hidden_states)\n",
        "\n",
        "        return logits"
      ],
      "metadata": {
        "id": "aAB9yLHBwLq8"
      },
      "execution_count": 206,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train"
      ],
      "metadata": {
        "id": "Y9Nl2zQzF0PB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_dataloader, valid_dataloader=None, epochs=2):\n",
        "        \n",
        "        # train_dataloaer 학습을 epochs만큼 반복\n",
        "        for epoch in range(epochs):\n",
        "            print(f\"*****Epoch {epoch} Train Start*****\")\n",
        "            \n",
        "            # 배치 단위 평균 loss와 총 평균 loss 계산하기위해 변수 생성\n",
        "            total_loss, batch_loss, batch_count = 0,0,0\n",
        "        \n",
        "            # model을 train 모드로 설정 & device 할당\n",
        "            model.train()\n",
        "            model.to(device)\n",
        "            \n",
        "            # data iterator를 돌면서 하나씩 학습\n",
        "            for step, batch in enumerate(train_dataloader):\n",
        "                batch_count+=1\n",
        "                \n",
        "                # tensor 연산 전, 각 tensor에 device 할당\n",
        "                batch = tuple(item.to(device) for item in batch)\n",
        "            \n",
        "                batch_input, batch_label = batch\n",
        "            \n",
        "                # batch마다 모델이 갖고 있는 기존 gradient를 초기화\n",
        "                model.zero_grad()\n",
        "            \n",
        "                # forward\n",
        "                logits = model(**batch_input)\n",
        "            \n",
        "                # loss\n",
        "                loss = loss_fct(logits, batch_label)\n",
        "                batch_loss += loss.item()\n",
        "                total_loss += loss.item()\n",
        "            \n",
        "                # backward -> 파라미터의 미분(gradient)를 자동으로 계산\n",
        "                loss.backward()\n",
        "                \n",
        "                # gradient clipping 적용 \n",
        "                clip_grad_norm_(model.parameters(), 1.0)\n",
        "                \n",
        "                # optimizer & scheduler 업데이트\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "                \n",
        "                # 배치 10개씩 처리할 때마다 평균 loss와 lr를 출력\n",
        "                if (step % 10 == 0 and step != 0):\n",
        "                    learning_rate = optimizer.param_groups[0]['lr']\n",
        "                    print(f\"Epoch: {epoch}, Step : {step}, LR : {learning_rate}, Avg Loss : {batch_loss / batch_count:.4f}\")\n",
        "\n",
        "                    # reset \n",
        "                    batch_loss, batch_count = 0,0\n",
        "            \n",
        "            print(f\"Epoch {epoch} Total Mean Loss : {total_loss/(step+1):.4f}\")\n",
        "            print(f\"*****Epoch {epoch} Train Finish*****\\n\")\n",
        "            \n",
        "            if valid_dataloader is not None:\n",
        "                print(f\"*****Epoch {epoch} Valid Start*****\")\n",
        "                valid_loss, valid_acc = validate(model, valid_dataloader)\n",
        "                print(f\"Epoch {epoch} Valid Loss : {valid_loss:.4f} Valid Acc : {valid_acc:.2f}\")\n",
        "                print(f\"*****Epoch {epoch} Valid Finish*****\\n\")\n",
        "            \n",
        "            # checkpoint 저장\n",
        "            save_checkpoint(\".\", model, optimizer, scheduler, epoch, total_loss/(step+1))\n",
        "                \n",
        "        print(\"Train Completed. End Program.\")"
      ],
      "metadata": {
        "id": "kK09khnhJQfo"
      },
      "execution_count": 240,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fct = CrossEntropyLoss()\n",
        "epochs=4\n",
        "model, optimizer, scheduler = initializer(train_dataloader, epochs)\n",
        "train_model(model, train_dataloader, valid_dataloader, epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4QLtohXGJUJF",
        "outputId": "d1b9a550-4ff9-4015-8a7e-dc0d67ec0728"
      },
      "execution_count": 241,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total train steps with 4 epochs: 1316\n",
            "*****Epoch 0 Train Start*****\n",
            "Epoch: 0, Step : 10, LR : 1.9832826747720366e-05, Avg Loss : 1.6091\n",
            "Epoch: 0, Step : 20, LR : 1.968085106382979e-05, Avg Loss : 1.5229\n",
            "Epoch: 0, Step : 30, LR : 1.952887537993921e-05, Avg Loss : 1.5430\n",
            "Epoch: 0, Step : 40, LR : 1.9376899696048635e-05, Avg Loss : 1.5405\n",
            "Epoch: 0, Step : 50, LR : 1.922492401215806e-05, Avg Loss : 1.4104\n",
            "Epoch: 0, Step : 60, LR : 1.9072948328267476e-05, Avg Loss : 1.4144\n",
            "Epoch: 0, Step : 70, LR : 1.89209726443769e-05, Avg Loss : 1.4123\n",
            "Epoch: 0, Step : 80, LR : 1.8768996960486324e-05, Avg Loss : 1.4046\n",
            "Epoch: 0, Step : 90, LR : 1.8617021276595745e-05, Avg Loss : 1.3446\n",
            "Epoch: 0, Step : 100, LR : 1.846504559270517e-05, Avg Loss : 1.3383\n",
            "Epoch: 0, Step : 110, LR : 1.8313069908814593e-05, Avg Loss : 1.3015\n",
            "Epoch: 0, Step : 120, LR : 1.8161094224924014e-05, Avg Loss : 1.3150\n",
            "Epoch: 0, Step : 130, LR : 1.8009118541033438e-05, Avg Loss : 1.2733\n",
            "Epoch: 0, Step : 140, LR : 1.785714285714286e-05, Avg Loss : 1.2474\n",
            "Epoch: 0, Step : 150, LR : 1.770516717325228e-05, Avg Loss : 1.1517\n",
            "Epoch: 0, Step : 160, LR : 1.7553191489361703e-05, Avg Loss : 1.1611\n",
            "Epoch: 0, Step : 170, LR : 1.7401215805471124e-05, Avg Loss : 1.2041\n",
            "Epoch: 0, Step : 180, LR : 1.7249240121580548e-05, Avg Loss : 1.1586\n",
            "Epoch: 0, Step : 190, LR : 1.7097264437689972e-05, Avg Loss : 1.1921\n",
            "Epoch: 0, Step : 200, LR : 1.6945288753799393e-05, Avg Loss : 1.2385\n",
            "Epoch: 0, Step : 210, LR : 1.6793313069908817e-05, Avg Loss : 1.1717\n",
            "Epoch: 0, Step : 220, LR : 1.6641337386018238e-05, Avg Loss : 1.1672\n",
            "Epoch: 0, Step : 230, LR : 1.648936170212766e-05, Avg Loss : 1.1658\n",
            "Epoch: 0, Step : 240, LR : 1.6337386018237082e-05, Avg Loss : 1.1326\n",
            "Epoch: 0, Step : 250, LR : 1.6185410334346506e-05, Avg Loss : 1.2073\n",
            "Epoch: 0, Step : 260, LR : 1.6033434650455927e-05, Avg Loss : 1.1210\n",
            "Epoch: 0, Step : 270, LR : 1.588145896656535e-05, Avg Loss : 1.1215\n",
            "Epoch: 0, Step : 280, LR : 1.5729483282674772e-05, Avg Loss : 1.1924\n",
            "Epoch: 0, Step : 290, LR : 1.5577507598784196e-05, Avg Loss : 1.1540\n",
            "Epoch: 0, Step : 300, LR : 1.542553191489362e-05, Avg Loss : 1.1389\n",
            "Epoch: 0, Step : 310, LR : 1.527355623100304e-05, Avg Loss : 1.2145\n",
            "Epoch: 0, Step : 320, LR : 1.5121580547112461e-05, Avg Loss : 1.1174\n",
            "Epoch 0 Total Mean Loss : 1.2672\n",
            "*****Epoch 0 Train Finish*****\n",
            "\n",
            "*****Epoch 0 Valid Start*****\n",
            "Epoch 0 Valid Loss : 1.1166 Valid Acc : 63.21\n",
            "*****Epoch 0 Valid Finish*****\n",
            "\n",
            "Saving epoch 0 checkpoint at ./model.ckpt.0\n",
            "*****Epoch 1 Train Start*****\n",
            "Epoch: 1, Step : 10, LR : 1.4832826747720366e-05, Avg Loss : 1.1196\n",
            "Epoch: 1, Step : 20, LR : 1.4680851063829789e-05, Avg Loss : 1.0759\n",
            "Epoch: 1, Step : 30, LR : 1.4528875379939211e-05, Avg Loss : 0.9583\n",
            "Epoch: 1, Step : 40, LR : 1.4376899696048633e-05, Avg Loss : 1.1062\n",
            "Epoch: 1, Step : 50, LR : 1.4224924012158057e-05, Avg Loss : 1.1022\n",
            "Epoch: 1, Step : 60, LR : 1.4072948328267476e-05, Avg Loss : 1.0873\n",
            "Epoch: 1, Step : 70, LR : 1.39209726443769e-05, Avg Loss : 1.0861\n",
            "Epoch: 1, Step : 80, LR : 1.3768996960486323e-05, Avg Loss : 0.9896\n",
            "Epoch: 1, Step : 90, LR : 1.3617021276595745e-05, Avg Loss : 1.0286\n",
            "Epoch: 1, Step : 100, LR : 1.3465045592705168e-05, Avg Loss : 1.0797\n",
            "Epoch: 1, Step : 110, LR : 1.3313069908814592e-05, Avg Loss : 1.0361\n",
            "Epoch: 1, Step : 120, LR : 1.3161094224924014e-05, Avg Loss : 1.0226\n",
            "Epoch: 1, Step : 130, LR : 1.3009118541033437e-05, Avg Loss : 1.0305\n",
            "Epoch: 1, Step : 140, LR : 1.2857142857142859e-05, Avg Loss : 1.0435\n",
            "Epoch: 1, Step : 150, LR : 1.270516717325228e-05, Avg Loss : 1.0634\n",
            "Epoch: 1, Step : 160, LR : 1.2553191489361702e-05, Avg Loss : 1.0626\n",
            "Epoch: 1, Step : 170, LR : 1.2401215805471124e-05, Avg Loss : 0.9418\n",
            "Epoch: 1, Step : 180, LR : 1.2249240121580548e-05, Avg Loss : 0.9911\n",
            "Epoch: 1, Step : 190, LR : 1.2097264437689971e-05, Avg Loss : 1.0500\n",
            "Epoch: 1, Step : 200, LR : 1.1945288753799393e-05, Avg Loss : 1.0142\n",
            "Epoch: 1, Step : 210, LR : 1.1793313069908816e-05, Avg Loss : 0.9819\n",
            "Epoch: 1, Step : 220, LR : 1.1641337386018238e-05, Avg Loss : 1.0225\n",
            "Epoch: 1, Step : 230, LR : 1.1489361702127662e-05, Avg Loss : 1.0535\n",
            "Epoch: 1, Step : 240, LR : 1.1337386018237083e-05, Avg Loss : 1.0072\n",
            "Epoch: 1, Step : 250, LR : 1.1185410334346505e-05, Avg Loss : 1.0197\n",
            "Epoch: 1, Step : 260, LR : 1.1033434650455928e-05, Avg Loss : 1.0263\n",
            "Epoch: 1, Step : 270, LR : 1.088145896656535e-05, Avg Loss : 1.0458\n",
            "Epoch: 1, Step : 280, LR : 1.0729483282674772e-05, Avg Loss : 0.9517\n",
            "Epoch: 1, Step : 290, LR : 1.0577507598784196e-05, Avg Loss : 1.0548\n",
            "Epoch: 1, Step : 300, LR : 1.0425531914893619e-05, Avg Loss : 1.0298\n",
            "Epoch: 1, Step : 310, LR : 1.0273556231003041e-05, Avg Loss : 1.0101\n",
            "Epoch: 1, Step : 320, LR : 1.0121580547112462e-05, Avg Loss : 1.1243\n",
            "Epoch 1 Total Mean Loss : 1.0368\n",
            "*****Epoch 1 Train Finish*****\n",
            "\n",
            "*****Epoch 1 Valid Start*****\n",
            "Epoch 1 Valid Loss : 1.0833 Valid Acc : 67.82\n",
            "*****Epoch 1 Valid Finish*****\n",
            "\n",
            "Saving epoch 1 checkpoint at ./model.ckpt.1\n",
            "*****Epoch 2 Train Start*****\n",
            "Epoch: 2, Step : 10, LR : 9.832826747720365e-06, Avg Loss : 0.9528\n",
            "Epoch: 2, Step : 20, LR : 9.680851063829787e-06, Avg Loss : 0.9821\n",
            "Epoch: 2, Step : 30, LR : 9.528875379939211e-06, Avg Loss : 0.9057\n",
            "Epoch: 2, Step : 40, LR : 9.376899696048632e-06, Avg Loss : 0.9710\n",
            "Epoch: 2, Step : 50, LR : 9.224924012158054e-06, Avg Loss : 0.8829\n",
            "Epoch: 2, Step : 60, LR : 9.072948328267479e-06, Avg Loss : 0.8618\n",
            "Epoch: 2, Step : 70, LR : 8.920972644376901e-06, Avg Loss : 0.9028\n",
            "Epoch: 2, Step : 80, LR : 8.768996960486323e-06, Avg Loss : 0.8729\n",
            "Epoch: 2, Step : 90, LR : 8.617021276595746e-06, Avg Loss : 0.9651\n",
            "Epoch: 2, Step : 100, LR : 8.465045592705168e-06, Avg Loss : 0.8556\n",
            "Epoch: 2, Step : 110, LR : 8.31306990881459e-06, Avg Loss : 0.8579\n",
            "Epoch: 2, Step : 120, LR : 8.161094224924013e-06, Avg Loss : 0.8168\n",
            "Epoch: 2, Step : 130, LR : 8.009118541033435e-06, Avg Loss : 0.9172\n",
            "Epoch: 2, Step : 140, LR : 7.857142857142858e-06, Avg Loss : 0.8817\n",
            "Epoch: 2, Step : 150, LR : 7.70516717325228e-06, Avg Loss : 0.9483\n",
            "Epoch: 2, Step : 160, LR : 7.553191489361703e-06, Avg Loss : 0.9041\n",
            "Epoch: 2, Step : 170, LR : 7.401215805471125e-06, Avg Loss : 0.8945\n",
            "Epoch: 2, Step : 180, LR : 7.249240121580547e-06, Avg Loss : 0.9099\n",
            "Epoch: 2, Step : 190, LR : 7.0972644376899704e-06, Avg Loss : 0.8677\n",
            "Epoch: 2, Step : 200, LR : 6.945288753799393e-06, Avg Loss : 0.8913\n",
            "Epoch: 2, Step : 210, LR : 6.793313069908816e-06, Avg Loss : 0.9267\n",
            "Epoch: 2, Step : 220, LR : 6.641337386018238e-06, Avg Loss : 0.9308\n",
            "Epoch: 2, Step : 230, LR : 6.48936170212766e-06, Avg Loss : 0.9015\n",
            "Epoch: 2, Step : 240, LR : 6.337386018237082e-06, Avg Loss : 0.9700\n",
            "Epoch: 2, Step : 250, LR : 6.185410334346506e-06, Avg Loss : 0.9031\n",
            "Epoch: 2, Step : 260, LR : 6.033434650455927e-06, Avg Loss : 0.9341\n",
            "Epoch: 2, Step : 270, LR : 5.8814589665653495e-06, Avg Loss : 0.8635\n",
            "Epoch: 2, Step : 280, LR : 5.729483282674773e-06, Avg Loss : 0.9395\n",
            "Epoch: 2, Step : 290, LR : 5.577507598784195e-06, Avg Loss : 0.8250\n",
            "Epoch: 2, Step : 300, LR : 5.425531914893617e-06, Avg Loss : 0.8613\n",
            "Epoch: 2, Step : 310, LR : 5.27355623100304e-06, Avg Loss : 0.8385\n",
            "Epoch: 2, Step : 320, LR : 5.121580547112462e-06, Avg Loss : 0.8951\n",
            "Epoch 2 Total Mean Loss : 0.9023\n",
            "*****Epoch 2 Train Finish*****\n",
            "\n",
            "*****Epoch 2 Valid Start*****\n",
            "Epoch 2 Valid Loss : 1.0718 Valid Acc : 71.52\n",
            "*****Epoch 2 Valid Finish*****\n",
            "\n",
            "Saving epoch 2 checkpoint at ./model.ckpt.2\n",
            "*****Epoch 3 Train Start*****\n",
            "Epoch: 3, Step : 10, LR : 4.832826747720365e-06, Avg Loss : 0.8521\n",
            "Epoch: 3, Step : 20, LR : 4.680851063829788e-06, Avg Loss : 0.8259\n",
            "Epoch: 3, Step : 30, LR : 4.52887537993921e-06, Avg Loss : 0.8716\n",
            "Epoch: 3, Step : 40, LR : 4.3768996960486325e-06, Avg Loss : 0.8921\n",
            "Epoch: 3, Step : 50, LR : 4.224924012158055e-06, Avg Loss : 0.8251\n",
            "Epoch: 3, Step : 60, LR : 4.072948328267477e-06, Avg Loss : 0.9155\n",
            "Epoch: 3, Step : 70, LR : 3.9209726443769005e-06, Avg Loss : 0.8056\n",
            "Epoch: 3, Step : 80, LR : 3.7689969604863225e-06, Avg Loss : 0.8196\n",
            "Epoch: 3, Step : 90, LR : 3.6170212765957453e-06, Avg Loss : 0.7773\n",
            "Epoch: 3, Step : 100, LR : 3.4650455927051673e-06, Avg Loss : 0.8634\n",
            "Epoch: 3, Step : 110, LR : 3.31306990881459e-06, Avg Loss : 0.7669\n",
            "Epoch: 3, Step : 120, LR : 3.1610942249240125e-06, Avg Loss : 0.7605\n",
            "Epoch: 3, Step : 130, LR : 3.009118541033435e-06, Avg Loss : 0.8067\n",
            "Epoch: 3, Step : 140, LR : 2.8571428571428573e-06, Avg Loss : 0.8369\n",
            "Epoch: 3, Step : 150, LR : 2.70516717325228e-06, Avg Loss : 0.9330\n",
            "Epoch: 3, Step : 160, LR : 2.553191489361702e-06, Avg Loss : 0.8727\n",
            "Epoch: 3, Step : 170, LR : 2.401215805471125e-06, Avg Loss : 0.7895\n",
            "Epoch: 3, Step : 180, LR : 2.2492401215805472e-06, Avg Loss : 0.8264\n",
            "Epoch: 3, Step : 190, LR : 2.0972644376899696e-06, Avg Loss : 0.8185\n",
            "Epoch: 3, Step : 200, LR : 1.9452887537993924e-06, Avg Loss : 0.8288\n",
            "Epoch: 3, Step : 210, LR : 1.7933130699088146e-06, Avg Loss : 0.8378\n",
            "Epoch: 3, Step : 220, LR : 1.6413373860182372e-06, Avg Loss : 0.8132\n",
            "Epoch: 3, Step : 230, LR : 1.4893617021276596e-06, Avg Loss : 0.7833\n",
            "Epoch: 3, Step : 240, LR : 1.3373860182370822e-06, Avg Loss : 0.8212\n",
            "Epoch: 3, Step : 250, LR : 1.1854103343465048e-06, Avg Loss : 0.8386\n",
            "Epoch: 3, Step : 260, LR : 1.0334346504559272e-06, Avg Loss : 0.8440\n",
            "Epoch: 3, Step : 270, LR : 8.814589665653496e-07, Avg Loss : 0.7916\n",
            "Epoch: 3, Step : 280, LR : 7.294832826747721e-07, Avg Loss : 0.8820\n",
            "Epoch: 3, Step : 290, LR : 5.775075987841945e-07, Avg Loss : 0.8680\n",
            "Epoch: 3, Step : 300, LR : 4.2553191489361704e-07, Avg Loss : 0.8144\n",
            "Epoch: 3, Step : 310, LR : 2.7355623100303953e-07, Avg Loss : 0.8237\n",
            "Epoch: 3, Step : 320, LR : 1.2158054711246203e-07, Avg Loss : 0.7661\n",
            "Epoch 3 Total Mean Loss : 0.8296\n",
            "*****Epoch 3 Train Finish*****\n",
            "\n",
            "*****Epoch 3 Valid Start*****\n",
            "Epoch 3 Valid Loss : 1.0887 Valid Acc : 72.01\n",
            "*****Epoch 3 Valid Finish*****\n",
            "\n",
            "Saving epoch 3 checkpoint at ./model.ckpt.3\n",
            "Train Completed. End Program.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "GFiQGULKrxfz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load checkpoint"
      ],
      "metadata": {
        "id": "4aim45otF4si"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_epoch = input()\n",
        "checkpoint = torch.load(f'./model.ckpt.{best_epoch}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4StQ5rCRPJt",
        "outputId": "9082c290-df51-4bf6-b248-9c1b3a4b3e35"
      },
      "execution_count": 243,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSy2twvbRSHd",
        "outputId": "3e733c92-5499-4904-fa84-e8a7edb98b76"
      },
      "execution_count": 244,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['epoch', 'model_state_dict', 'optimizer_state_dict', 'scheduler_state_dict', 'loss'])"
            ]
          },
          "metadata": {},
          "execution_count": 244
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs=1\n",
        "model, optimizer, scheduler = initializer(train_dataloader, epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kOuSODA6RSQ9",
        "outputId": "99bda740-c36c-40cc-f57d-cf5973fd0656"
      },
      "execution_count": 258,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total train steps with 1 epochs: 329\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(checkpoint[\"model_state_dict\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AAoxlaI_RSTb",
        "outputId": "9430bbf5-06c4-4594-8690-6b6da032fddb"
      },
      "execution_count": 259,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 259
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "predict and evaluate"
      ],
      "metadata": {
        "id": "w1uKfufLF8kC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model, test_dataloader):\n",
        "    \"\"\"\n",
        "    test_dataloader의 label별 확률값과 실제 label 값을 반환\n",
        "    \"\"\"\n",
        "\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    all_logits = []\n",
        "    all_labels = []\n",
        "\n",
        "    for step, batch in enumerate(test_dataloader):\n",
        "        print(f\"{step}/{len(test_dataloader)}\")\n",
        "        \n",
        "        batch_input, batch_label = batch\n",
        "        \n",
        "        batch_input = batch_input.to(device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            logits = model(**batch_input)\n",
        "            all_logits.append(logits)\n",
        "        all_labels.extend(batch_label)\n",
        "\n",
        "    all_logits = torch.cat(all_logits, dim=0)\n",
        "    probs = F.softmax(all_logits, dim=1).cpu().numpy()\n",
        "    all_labels = np.array(all_labels)\n",
        "\n",
        "    return probs, all_labels\n"
      ],
      "metadata": {
        "id": "UhtXfy8kJvMR"
      },
      "execution_count": 247,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "probs, labels = predict(model, test_dataloader)"
      ],
      "metadata": {
        "id": "NQAuNkxRRdxw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07c176d5-ae0a-4bf7-e348-69317fe9645b"
      },
      "execution_count": 248,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0/9\n",
            "1/9\n",
            "2/9\n",
            "3/9\n",
            "4/9\n",
            "5/9\n",
            "6/9\n",
            "7/9\n",
            "8/9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "from scipy.stats import pearsonr\n",
        "def evaluate(pred, test=df_test0):\n",
        "    pred_binary = list(map(lambda x: 1 if x>=3 else 0, pred))\n",
        "    test_real_label = test['labels'].map(lambda x: x['real-label'])\n",
        "    test_binary_label = test['labels'].map(lambda x: x['binary-label'])\n",
        "    print(f'acc : {accuracy_score(test_binary_label, pred_binary)}')\n",
        "    print(f'f1 : {f1_score(test_binary_label, pred_binary)}')\n",
        "    print(f'pearson : {pearsonr(test_real_label, pred)}')\n",
        "    "
      ],
      "metadata": {
        "id": "r_CM_p_SzS3t"
      },
      "execution_count": 249,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate(labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sv_HfFaARi6o",
        "outputId": "381205b4-1cae-4a0a-fb0f-36333e37b7b7"
      },
      "execution_count": 250,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "acc : 1.0\n",
            "f1 : 1.0\n",
            "pearson : (0.9819793453996939, 0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "output"
      ],
      "metadata": {
        "id": "UwfT52-LGDrR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = pd.DataFrame(labels,columns=['pred_real_label'])\n",
        "filename = input()\n",
        "output.to_csv(f'{filename}.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWOiLVeOGFay",
        "outputId": "54584bc1-2c3f-40a0-a3d2-d1f1597fc21d"
      },
      "execution_count": 251,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "second_test\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_test['sentence'][0]"
      ],
      "metadata": {
        "id": "B5N_unJk427o",
        "outputId": "26cc4827-f623-4c00-88c6-4602f5d21205",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "execution_count": 256,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'무엇보다도 호스트분들이 너무 친절하셨습니다. [SEP] 무엇보다도, 호스트들은 매우 친절했습니다.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 256
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = df_test['sentence'][0]\n",
        "tokenizer_bert = BertTokenizer.from_pretrained(\"klue/bert-base\")\n",
        "\n",
        "tensorized_input = tokenizer_bert(\n",
        "    text,\n",
        "    add_special_tokens=True,\n",
        "    padding=\"longest\", \n",
        "    truncation=True, \n",
        "    max_length=512,\n",
        "    return_tensors='pt' \n",
        ")\n",
        "tensorized_input.to(device)\n",
        "model.eval()\n",
        "model.to(device)\n",
        "traced_model = torch.jit.trace(model, tensorized_input['input_ids'])\n",
        "torch.jit.save(traced_model, \"model_scripted.pt\")"
      ],
      "metadata": {
        "id": "EVR2J_0cj8AE"
      },
      "execution_count": 260,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.jit.load('model_scripted.pt')"
      ],
      "metadata": {
        "id": "LSjOaAsdWyT4"
      },
      "execution_count": 261,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model(tensorized_input['input_ids'])"
      ],
      "metadata": {
        "id": "1E0y8y1PqQc-",
        "outputId": "e6c34aa5-bdd6-40f2-dd4d-06af93aef6c8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 262,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-4.1509,  0.7839, -1.1482, -2.2241,  2.3314, -2.9184]],\n",
              "       device='cuda:0', grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 262
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_test['label'][0]"
      ],
      "metadata": {
        "id": "2BJfmFIL48Q0",
        "outputId": "b197e9b2-f8a0-459b-e968-cced2879ca81",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 257,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 257
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "STS.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}